{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maneeha/KGLLM/blob/main/Dataset%20F500_words_sentence_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wcmatch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnNa81yn3tIL",
        "outputId": "dc8fee82-41b0-4d8a-9f29-38f4909fb52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wcmatch\n",
            "  Downloading wcmatch-8.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting bracex>=2.1.1 (from wcmatch)\n",
            "  Downloading bracex-2.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: bracex, wcmatch\n",
            "Successfully installed bracex-2.4 wcmatch-8.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oHpYDZA73ZAz"
      },
      "outputs": [],
      "source": [
        "from wcmatch import wcmatch\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "# function to extract sentences from text files\n",
        "def extract_sentences(file_path):\n",
        "    # This is what will be returned from the function with Empty as a placeholder\n",
        "    final_content = \"Empty\"\n",
        "    # The files contents will be saved in the variable below for processing\n",
        "    content = ''\n",
        "    # Stride length is the maximum number of words we want to include in our sequence being generated\n",
        "    stride = 500\n",
        "\n",
        "    # Validate file path and return \"Empty\" if not valid\n",
        "    if not os.path.isfile(file_path):\n",
        "        print(\"{} does not exist \".format(file_path))\n",
        "        return final_content\n",
        "\n",
        "    # Read file and remove empty line and new lines\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file.readlines():\n",
        "            if line.strip():\n",
        "                if len(line.strip()) > 2:\n",
        "                    content += line.replace('\\n','')\n",
        "\n",
        "    # Create list of words and generate number of words\n",
        "    split_content = content.split()\n",
        "    seq_len = len(split_content)\n",
        "\n",
        "    # Check that contents have been extracted and reset Empty flag\n",
        "    if seq_len > 0:\n",
        "        final_content = \"\"\n",
        "\n",
        "    # Create the sequences\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "        end_loc = min(begin_loc + stride, seq_len)\n",
        "        if len(split_content[begin_loc:end_loc]) != 0:\n",
        "            # Include a new line at the end of the generated sequence\n",
        "            final_content += \"<s>\" + ' '.join(split_content[begin_loc:end_loc]) + \"</s> \\n\"\n",
        "\n",
        "    # Return the sequences\n",
        "    return final_content"
      ],
      "metadata": {
        "id": "vhGKktdp3ZA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "def create_dataset(folder_path, file_ext, folder_destination, dataset_name=\"dataset.txt\"):\n",
        "    # folder_path is the source path\n",
        "    # file_ext is the file formats to be matched\n",
        "    # folder_destination is where to save the dataset\n",
        "    # dataset_name is the name of the dataset (default will be dataset.txt)\n",
        "\n",
        "    # Save file paths matched in files variable\n",
        "    print(\"Start processing ...\")\n",
        "    files = wcmatch.WcMatch(root_dir=folder_path, file_pattern=file_ext, flags=wcmatch.RECURSIVE).match()\n",
        "    print(str(len(files)) + \" files to be processed!\")\n",
        "\n",
        "    # Loop through and process each file\n",
        "    i = 0\n",
        "    while i < len(files):\n",
        "        try:\n",
        "            # Get extracted sentences\n",
        "            contents = extract_sentences(files[i])\n",
        "            # Ignore \"Empty\" sentences\n",
        "            if contents != 'Empty':\n",
        "                # Open or create dataset in append and byte mode\n",
        "                f = open(folder_destination + dataset_name, \"ab\")\n",
        "                # Save contents in utf-8 encoding\n",
        "                f.write(contents.encode('utf-8'))\n",
        "                f.close()\n",
        "        except Exception as e:\n",
        "            # Log any issues encountered for further investigation\n",
        "            print(files[i])\n",
        "            print (\"Error saving extraction to file \" + str(e))\n",
        "\n",
        "        # Increment counter\n",
        "        i += 1\n",
        "\n",
        "    print(\"Finished processing ...\")"
      ],
      "metadata": {
        "id": "hFVCivpm3ZA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "# Read from the following path\n",
        "folder_path = \"/content/drive/My Drive/Colab Notebooks/pdf\"\n",
        "#Only process the following file formats, add more file extensions using comma separation\n",
        "file_ext = \"*.txt\"\n",
        "# Save to the following path\n",
        "folder_destination = \"/content/drive/My Drive/Colab Notebooks/pdf/extracted\"\n",
        "# Dataset name (can be omitted to use default values)\n",
        "dataset_name=\"custom-llama2-dataset.txt\""
      ],
      "metadata": {
        "id": "jBBNZh7T3ZA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing ...\n",
            "0 files to be processed!\n",
            "Finished processing ...\n"
          ]
        }
      ],
      "source": [
        "create_dataset(folder_path, file_ext, folder_destination, dataset_name)"
      ],
      "metadata": {
        "id": "NtF5LKHZ3ZA8",
        "outputId": "68833f64-87fb-43be-fac3-64cfb3f7ce86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "lISfC2Z53ZA-",
        "outputId": "eaa9a797-d1db-4552-d64c-2aad6241f009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "Q2K5rGJN3ZA-"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}